{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NandhiniAnne/nandhiniresumechatbot/blob/main/Resume_Chatbot_%26_Semantic_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import fitz  # PyMuPDF\n",
        "import json\n",
        "import os\n",
        "import asyncio\n",
        "import aiohttp\n",
        "GEMINI_API_KEY = \"AIzaSyBrSqwcNmaDUVoEFHVH4m1qjfdKorLaSEQ\"\n",
        "# --- 1. Data Loading and Preprocessing ---\n",
        "def load_and_parse_resume_by_line(pdf_path, filename):\n",
        "    \"\"\"Loads a single PDF and parses it line-by-line to create clean sections.\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        full_text = \"\".join(page.get_text() for page in doc)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF {filename}: {e}\")\n",
        "        return []\n",
        "\n",
        "    headings = [\n",
        "        'OBJECTIVE', 'EDUCATION', 'SKILLS', 'PROJECTS', 'CERTIFICATIONS',\n",
        "        'EXTRACURRICULAR ACTIVITIES', 'ADDITIONAL INFORMATION', 'WORK EXPERIENCE',\n",
        "        'EXPERIENCE', 'EMPLOYMENT HISTORY', 'PUBLICATIONS', 'SUMMARY', 'CONTACT'\n",
        "    ]\n",
        "    lines = full_text.split('\\n')\n",
        "    chunks = []\n",
        "    current_chunk_lines = []\n",
        "    header_chunk_finished = False\n",
        "\n",
        "    for line in lines:\n",
        "        cleaned_line = line.strip()\n",
        "        is_heading = cleaned_line.upper() in headings\n",
        "\n",
        "        if is_heading and not header_chunk_finished:\n",
        "            if current_chunk_lines: chunks.append(\"\\n\".join(current_chunk_lines).strip())\n",
        "            current_chunk_lines = [cleaned_line]\n",
        "            header_chunk_finished = True\n",
        "        elif is_heading and header_chunk_finished:\n",
        "            if current_chunk_lines: chunks.append(\"\\n\".join(current_chunk_lines).strip())\n",
        "            current_chunk_lines = [cleaned_line]\n",
        "        else:\n",
        "            if cleaned_line: current_chunk_lines.append(cleaned_line)\n",
        "\n",
        "    if current_chunk_lines: chunks.append(\"\\n\".join(current_chunk_lines).strip())\n",
        "    return [{'source': filename, 'content': chunk} for chunk in chunks if len(chunk) > 20]\n",
        "\n",
        "# --- 2. Semantic Embedding and Indexing ---\n",
        "def build_semantic_index(chunks_with_source, model):\n",
        "    \"\"\"Builds a FAISS index from a list of chunk dictionaries.\"\"\"\n",
        "    if not chunks_with_source:\n",
        "        return None\n",
        "    text_chunks = [chunk['content'] for chunk in chunks_with_source]\n",
        "    embeddings = model.encode(text_chunks, convert_to_tensor=True)\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(embeddings.cpu().numpy())\n",
        "    return index\n",
        "\n",
        "# --- 3. Semantic Search ---\n",
        "def search(query, model, index, all_chunks_with_source, top_k=3):\n",
        "    \"\"\"Searches the index and returns the original chunk dictionaries.\"\"\"\n",
        "    if index is None:\n",
        "        return []\n",
        "    query_embedding = model.encode([query])\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    return [all_chunks_with_source[i] for i in indices[0]]\n",
        "\n",
        "# --- 4. Question Answering with Gemini API ---\n",
        "async def answer_question_with_gemini(question, context_chunks):\n",
        "    \"\"\"Uses the Gemini 2.5 Flash model via the provided API key.\"\"\"\n",
        "    api_key = GEMINI_API_KEY\n",
        "    if not api_key or api_key == \"PASTE_YOUR_NEW_API_KEY_HERE\":\n",
        "        return \"ERROR: Gemini API key is missing. Please paste your key at the top of the python script.\"\n",
        "\n",
        "    if not context_chunks:\n",
        "        return \"No relevant information found across the uploaded resumes.\"\n",
        "\n",
        "    context = \"\\n\\n---\\n\\n\".join([f\"CONTEXT from resume '{c['source']}':\\n{c['content']}\" for c in context_chunks])\n",
        "    system_prompt = \"You are an expert HR assistant. Answer questions strictly based on the provided resume context. If information is missing, state that clearly. For lists like skills, use bullet points.\"\n",
        "    user_prompt = f\"{context}\\n\\nQUESTION: {question}\"\n",
        "\n",
        "    api_url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={api_key}\"\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": user_prompt}]}],\n",
        "        \"systemInstruction\": {\"parts\": [{\"text\": system_prompt}]},\n",
        "        \"generationConfig\": {\"temperature\": 0.1, \"topP\": 0.9, \"maxOutputTokens\": 800}\n",
        "    }\n",
        "\n",
        "    max_retries = 3\n",
        "    delay = 1\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                async with session.post(api_url, json=payload, headers={'Content-Type': 'application/json'}) as response:\n",
        "                    if response.status == 200:\n",
        "                        result = await response.json()\n",
        "                        candidate = result.get('candidates', [{}])[0]\n",
        "                        if candidate.get('content', {}).get('parts', [{}])[0].get('text'):\n",
        "                            return candidate['content']['parts'][0]['text']\n",
        "                        else:\n",
        "                            return f\"AI returned an empty response. Reason: {candidate.get('finishReason', 'Unknown Error')}\"\n",
        "                    elif response.status == 429:\n",
        "                        print(f\"Rate limited. Retrying in {delay}s...\")\n",
        "                        await asyncio.sleep(delay)\n",
        "                        delay *= 2\n",
        "                    else:\n",
        "                        error_text = await response.text()\n",
        "                        print(f\"API Error: {response.status} - {error_text}\")\n",
        "                        return f\"Error: API request failed with status: {response.status}. Check your API key and billing.\"\n",
        "            except aiohttp.ClientError as e:\n",
        "                print(f\"Network Error (Attempt {attempt+1}/{max_retries}): {e}\")\n",
        "                await asyncio.sleep(delay)\n",
        "                delay *= 2\n",
        "\n",
        "    return \"Sorry, the AI service is currently unavailable. Please try again later.\"\n",
        "\n",
        "\n",
        "# --- Main Application Logic & Gradio UI ---\n",
        "class MultiResumeChatbot:\n",
        "    def __init__(self):\n",
        "        print(\"Loading embedding model...\")\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.all_chunks_with_source = []\n",
        "        self.faiss_index = None\n",
        "        print(\"Model loaded.\")\n",
        "\n",
        "    def index_resumes(self, file_paths):\n",
        "        if not file_paths: return \"Please upload at least one resume file.\"\n",
        "        self.all_chunks_with_source = []\n",
        "        for temp_file in file_paths:\n",
        "            filename = os.path.basename(temp_file.name)\n",
        "            print(f\"Parsing '{filename}'...\")\n",
        "            chunks = load_and_parse_resume_by_line(temp_file.name, filename)\n",
        "            self.all_chunks_with_source.extend(chunks)\n",
        "        if not self.all_chunks_with_source:\n",
        "             return \"Could not parse any text from the provided files.\"\n",
        "        print(f\"Building index for {len(self.all_chunks_with_source)} chunks...\")\n",
        "        self.faiss_index = build_semantic_index(self.all_chunks_with_source, self.embedding_model)\n",
        "        return f\"Successfully indexed {len(file_paths)} resumes. Ready for questions.\"\n",
        "\n",
        "    async def query(self, question):\n",
        "        if self.faiss_index is None:\n",
        "            return \"Please upload and index resumes first.\"\n",
        "        relevant_chunks = search(question, self.embedding_model, self.faiss_index, self.all_chunks_with_source)\n",
        "        return await answer_question_with_gemini(question, relevant_chunks)\n",
        "\n",
        "def create_chatbot_interface():\n",
        "    chatbot_instance = MultiResumeChatbot()\n",
        "    with gr.Blocks(theme=gr.themes.Soft(), title=\"Multi-Resume Chatbot\") as demo:\n",
        "        gr.Markdown(\"# AI-Powered Multi-Resume Chatbot (Gemini Edition)\")\n",
        "        gr.Markdown(\"Upload one or more resumes, click 'Index Resumes', then ask your questions.\")\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                file_uploader = gr.File(label=\"Upload Resumes\", file_count=\"multiple\", file_types=[\".pdf\"])\n",
        "                index_button = gr.Button(\"Index Resumes\", variant=\"primary\")\n",
        "                status_output = gr.Markdown(value=\"*No resumes indexed yet.*\")\n",
        "            with gr.Column(scale=2):\n",
        "                chatbot = gr.Chatbot(label=\"Chat History\", height=550)\n",
        "                msg = gr.Textbox(label=\"Your Question\", placeholder=\"e.g., Compare the skills of the candidates...\", interactive=False)\n",
        "                clear = gr.Button(\"Clear Chat\")\n",
        "        def handle_indexing(files):\n",
        "            if not files: return \"Please upload files first.\", gr.Textbox(interactive=False)\n",
        "            status = chatbot_instance.index_resumes(files)\n",
        "            return status, gr.Textbox(interactive=True)\n",
        "        async def user(user_message, history):\n",
        "            return \"\", history + [[user_message, None]]\n",
        "        async def bot(history):\n",
        "            user_message = history[-1][0]\n",
        "            bot_message = await chatbot_instance.query(user_message)\n",
        "            history[-1][1] = bot_message\n",
        "            return history\n",
        "        index_button.click(handle_indexing, inputs=[file_uploader], outputs=[status_output, msg])\n",
        "        msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(bot, chatbot, chatbot)\n",
        "        def clear_chat(): return None, \"\"\n",
        "        clear.click(clear_chat, None, [chatbot, msg], queue=False)\n",
        "    return demo\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if GEMINI_API_KEY == \"PASTE_YOUR_NEW_API_KEY_HERE\":\n",
        "        print(\"=\"*50)\n",
        "        print(\"WARNING: You haven't added your API key to the script yet!\")\n",
        "        print(\"Please edit the 'multi_resume_chatbot.py' file and paste your key.\")\n",
        "        print(\"=\"*50)\n",
        "    try:\n",
        "        interface = create_chatbot_interface()\n",
        "        interface.launch(share=True, debug=True)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model...\n",
            "Model loaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-809122334.py:162: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"Chat History\", height=550)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://57376791d522c50545.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://57376791d522c50545.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing 'Nandhini_Resume.pdf'...\n",
            "Building index for 8 chunks...\n",
            "Parsing 'Nandhini_Resume.pdf'...\n",
            "Building index for 8 chunks...\n",
            "Parsing 'Nandhini_Resume.pdf'...\n",
            "Parsing 'Surya_Vardhan_Resume_new1.pdf'...\n",
            "Building index for 16 chunks...\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://57376791d522c50545.gradio.live\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "Nq3cQCWP_wS-",
        "outputId": "8346dc2d-0c8c-42bc-d0f6-67841b954cf4"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}