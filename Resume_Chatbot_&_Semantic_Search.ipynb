{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NandhiniAnne/nandhiniresumechatbot/blob/main/Resume_Chatbot_%26_Semantic_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- AI-Powered Multi-Resume Chatbot ---\n",
        "# This script launches a Gradio web interface to analyze multiple resumes.\n",
        "# It uses semantic search to find relevant information and the Gemini API\n",
        "# to provide high-quality, synthesized answers to user questions.\n",
        "\n",
        "# --- Step 1: Import necessary libraries ---\n",
        "import gradio as gr\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import fitz  # PyMuPDF\n",
        "import json\n",
        "import os\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import numpy as np\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# --- Step 2: Securely load API Key ---\n",
        "# This looks for a .env file in the same directory and loads the key.\n",
        "# Make sure your .env file has: GEMINI_API_KEY=\"YOUR_SECRET_KEY\"\n",
        "load_dotenv()\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "\n",
        "# --- 3. Core Functions: Parsing, Indexing, Searching, and AI Answering ---\n",
        "\n",
        "def load_and_parse_resume_by_line(pdf_path, filename):\n",
        "    \"\"\"Loads a single PDF and parses it line-by-line to create clean sections.\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        full_text = \"\".join(page.get_text() for page in doc)\n",
        "        doc.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF {filename}: {e}\")\n",
        "        return []\n",
        "\n",
        "    headings = [\n",
        "        'OBJECTIVE', 'EDUCATION', 'SKILLS', 'PROJECTS', 'CERTIFICATIONS',\n",
        "        'EXTRACURRICULAR ACTIVITIES', 'ADDITIONAL INFORMATION', 'WORK EXPERIENCE',\n",
        "        'EXPERIENCE', 'EMPLOYMENT HISTORY', 'PUBLICATIONS', 'SUMMARY', 'CONTACT'\n",
        "    ]\n",
        "    lines = full_text.split('\\n')\n",
        "    chunks = []\n",
        "    current_chunk_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        cleaned_line = line.strip()\n",
        "        # Check if the line itself is a heading\n",
        "        is_heading = cleaned_line.upper() in headings\n",
        "\n",
        "        if is_heading:\n",
        "            # If we find a heading, the previous chunk is complete\n",
        "            if current_chunk_lines:\n",
        "                chunks.append(\"\\n\".join(current_chunk_lines).strip())\n",
        "            # Start a new chunk with the current heading\n",
        "            current_chunk_lines = [cleaned_line]\n",
        "        else:\n",
        "            # If it's not a heading, add the line to the current chunk\n",
        "            if cleaned_line:\n",
        "                current_chunk_lines.append(cleaned_line)\n",
        "\n",
        "    # Add the last remaining chunk\n",
        "    if current_chunk_lines:\n",
        "        chunks.append(\"\\n\".join(current_chunk_lines).strip())\n",
        "\n",
        "    return [{'source': filename, 'content': chunk} for chunk in chunks if len(chunk) > 20]\n",
        "\n",
        "def build_semantic_index(chunks_with_source, model):\n",
        "    \"\"\"Builds a FAISS index from a list of chunk dictionaries.\"\"\"\n",
        "    if not chunks_with_source:\n",
        "        return None, None\n",
        "    text_chunks = [chunk['content'] for chunk in chunks_with_source]\n",
        "    embeddings = model.encode(text_chunks)\n",
        "    embeddings_np = np.array(embeddings).astype('float32')\n",
        "    index = faiss.IndexFlatL2(embeddings_np.shape[1])\n",
        "    index.add(embeddings_np)\n",
        "    return index\n",
        "\n",
        "def search(query, model, index, all_chunks_with_source, top_k=3):\n",
        "    \"\"\"Searches the index and returns the original chunk dictionaries.\"\"\"\n",
        "    if index is None:\n",
        "        return []\n",
        "    query_embedding = model.encode([query])\n",
        "    query_embedding_np = np.array(query_embedding).astype('float32')\n",
        "    distances, indices = index.search(query_embedding_np, top_k)\n",
        "    return [all_chunks_with_source[i] for i in indices[0]]\n",
        "\n",
        "async def answer_question_with_gemini(question, context_chunks):\n",
        "    \"\"\"Uses the Gemini 2.5 Flash model via the loaded API key.\"\"\"\n",
        "    if not GEMINI_API_KEY:\n",
        "        return \"ERROR: Gemini API key is missing. Please create a .env file and add your GEMINI_API_KEY.\"\n",
        "\n",
        "    if not context_chunks:\n",
        "        return \"No relevant information found across the uploaded resumes.\"\n",
        "\n",
        "    context = \"\\n\\n---\\n\\n\".join([f\"CONTEXT from resume '{c['source']}':\\n{c['content']}\" for c in context_chunks])\n",
        "    system_prompt = \"You are an expert HR assistant. Answer questions strictly based on the provided resume context. If information is missing, state that clearly. For lists like skills, use bullet points.\"\n",
        "    user_prompt = f\"{context}\\n\\nQUESTION: {question}\"\n",
        "\n",
        "    api_url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={GEMINI_API_KEY}\"\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": user_prompt}]}],\n",
        "        \"systemInstruction\": {\"parts\": [{\"text\": system_prompt}]},\n",
        "        \"generationConfig\": {\"temperature\": 0.1, \"topP\": 0.9, \"maxOutputTokens\": 800}\n",
        "    }\n",
        "\n",
        "    max_retries = 3\n",
        "    delay = 1\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                async with session.post(api_url, json=payload, headers={'Content-Type': 'application/json'}) as response:\n",
        "                    if response.status == 200:\n",
        "                        result = await response.json()\n",
        "                        candidate = result.get('candidates', [{}])[0]\n",
        "                        if candidate.get('content', {}).get('parts', [{}])[0].get('text'):\n",
        "                            return candidate['content']['parts'][0]['text']\n",
        "                        else:\n",
        "                            return f\"AI returned an empty response. Reason: {candidate.get('finishReason', 'Unknown Error')}\"\n",
        "                    elif response.status == 429:\n",
        "                        print(f\"Rate limited. Retrying in {delay}s...\")\n",
        "                        await asyncio.sleep(delay)\n",
        "                        delay *= 2\n",
        "                    else:\n",
        "                        error_text = await response.text()\n",
        "                        print(f\"API Error: {response.status} - {error_text}\")\n",
        "                        return f\"Error: API request failed with status: {response.status}. Check your API key and billing.\"\n",
        "            except aiohttp.ClientError as e:\n",
        "                print(f\"Network Error (Attempt {attempt+1}/{max_retries}): {e}\")\n",
        "                await asyncio.sleep(delay)\n",
        "                delay *= 2\n",
        "\n",
        "    return \"Sorry, the AI service is currently unavailable. Please try again later.\"\n",
        "\n",
        "# --- 4. Main Application Logic & Gradio UI ---\n",
        "class MultiResumeChatbot:\n",
        "    \"\"\"A class to manage the state of the chatbot application.\"\"\"\n",
        "    def __init__(self):\n",
        "        print(\"Loading embedding model...\")\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.all_chunks_with_source = []\n",
        "        self.faiss_index = None\n",
        "        print(\"Model loaded. Ready to launch UI.\")\n",
        "\n",
        "    def index_resumes(self, file_paths):\n",
        "        \"\"\"Processes uploaded files and builds the search index.\"\"\"\n",
        "        if not file_paths:\n",
        "            return \"Please upload at least one resume file.\"\n",
        "\n",
        "        self.all_chunks_with_source = []\n",
        "        for temp_file in file_paths:\n",
        "            filename = os.path.basename(temp_file.name)\n",
        "            print(f\"Parsing '{filename}'...\")\n",
        "            chunks = load_and_parse_resume_by_line(temp_file.name, filename)\n",
        "            self.all_chunks_with_source.extend(chunks)\n",
        "\n",
        "        if not self.all_chunks_with_source:\n",
        "            return \"Could not parse any text from the provided files. Check PDF format.\"\n",
        "\n",
        "        print(f\"Building index for {len(self.all_chunks_with_source)} chunks...\")\n",
        "        self.faiss_index = build_semantic_index(self.all_chunks_with_source, self.embedding_model)\n",
        "        return f\"✅ Successfully indexed {len(file_paths)} resumes. Ready for questions.\"\n",
        "\n",
        "    async def query_and_answer(self, question):\n",
        "        \"\"\"Handles the search and generation process for a user query.\"\"\"\n",
        "        if self.faiss_index is None:\n",
        "            return \"Please upload and index resumes first.\"\n",
        "\n",
        "        print(f\"Searching for context for question: '{question}'\")\n",
        "        relevant_chunks = search(question, self.embedding_model, self.faiss_index, self.all_chunks_with_source)\n",
        "\n",
        "        print(\"Context found. Asking Gemini...\")\n",
        "        return await answer_question_with_gemini(question, relevant_chunks)\n",
        "\n",
        "def create_chatbot_interface():\n",
        "    \"\"\"Creates and configures the Gradio web interface.\"\"\"\n",
        "    chatbot_instance = MultiResumeChatbot()\n",
        "\n",
        "    with gr.Blocks(theme=gr.themes.Soft(), title=\"Multi-Resume Chatbot\") as demo:\n",
        "        gr.Markdown(\"<h1>🤖 AI-Powered Multi-Resume Chatbot</h1>\")\n",
        "        gr.Markdown(\"Upload one or more resumes, click 'Index Resumes', then ask your questions below.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                file_uploader = gr.File(label=\"Upload Resumes\", file_count=\"multiple\", file_types=[\".pdf\"])\n",
        "                index_button = gr.Button(\"Index Resumes\", variant=\"primary\", icon=\"📈\")\n",
        "                status_output = gr.Markdown(value=\"*No resumes indexed yet.*\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                chatbot = gr.Chatbot(label=\"Chat History\", height=550)\n",
        "                msg = gr.Textbox(label=\"Your Question\", placeholder=\"e.g., Compare the skills of the candidates...\", interactive=False)\n",
        "                clear = gr.Button(\"Clear Chat\", icon=\"🗑️\")\n",
        "\n",
        "        def handle_indexing(files):\n",
        "            if not files:\n",
        "                return \"Please upload files first.\", gr.Textbox(interactive=False)\n",
        "            status = chatbot_instance.index_resumes(files)\n",
        "            return status, gr.Textbox(interactive=True)\n",
        "\n",
        "        async def user(user_message, history):\n",
        "            return \"\", history + [[user_message, None]]\n",
        "\n",
        "        async def bot(history):\n",
        "            user_message = history[-1][0]\n",
        "            bot_message = await chatbot_instance.query_and_answer(user_message)\n",
        "            history[-1][1] = bot_message\n",
        "            return history\n",
        "\n",
        "        index_button.click(handle_indexing, inputs=[file_uploader], outputs=[status_output, msg])\n",
        "\n",
        "        msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
        "            bot, chatbot, chatbot\n",
        "        )\n",
        "\n",
        "        def clear_chat():\n",
        "            return None, \"\"\n",
        "\n",
        "        clear.click(clear_chat, None, [chatbot, msg], queue=False)\n",
        "\n",
        "    return demo\n",
        "\n",
        "# --- 5. Launch the Application ---\n",
        "if __name__ == \"__main__\":\n",
        "    if not GEMINI_API_KEY:\n",
        "        print(\"=\"*50)\n",
        "        print(\"🔴 WARNING: GEMINI_API_KEY not found in .env file!\")\n",
        "        print(\"Please create a file named .env and add your key.\")\n",
        "        print(\"Example .env content: GEMINI_API_KEY=\\\"AIzaSy...\\\"\")\n",
        "        print(\"=\"*50)\n",
        "    else:\n",
        "        try:\n",
        "            interface = create_chatbot_interface()\n",
        "            interface.launch(share=True, debug=True)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "🔴 WARNING: GEMINI_API_KEY not found in .env file!\n",
            "Please create a file named .env and add your key.\n",
            "Example .env content: GEMINI_API_KEY=\"AIzaSy...\"\n",
            "==================================================\n"
          ]
        }
      ],
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nq3cQCWP_wS-",
        "outputId": "47795b12-921a-4f13-b545-3f9739d010d5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}