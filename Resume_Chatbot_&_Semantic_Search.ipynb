{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NandhiniAnne/nandhiniresumechatbot/blob/main/Resume_Chatbot_%26_Semantic_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "\n",
        "# --- 1. Data Loading and Preprocessing (Unchanged) ---\n",
        "def load_and_parse_resume_by_line(pdf_path):\n",
        "    \"\"\"\n",
        "    Loads a PDF and parses it line-by-line to create the most accurate and clean sections.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        full_text = \"\".join(page.get_text() for page in doc)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF {pdf_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "    headings = [\n",
        "        'OBJECTIVE', 'EDUCATION', 'SKILLS', 'PROJECTS', 'CERTIFICATIONS',\n",
        "        'EXTRACURRICULAR ACTIVITIES', 'ADDITIONAL INFORMATION', 'WORK EXPERIENCE',\n",
        "        'EXPERIENCE', 'EMPLOYMENT HISTORY', 'PUBLICATIONS', 'SUMMARY'\n",
        "    ]\n",
        "    lines = full_text.split('\\n')\n",
        "    chunks = []\n",
        "    current_chunk_lines = []\n",
        "    header_chunk_finished = False\n",
        "\n",
        "    for line in lines:\n",
        "        cleaned_line = line.strip()\n",
        "        is_heading = cleaned_line.upper() in headings\n",
        "\n",
        "        if is_heading and not header_chunk_finished:\n",
        "            if current_chunk_lines:\n",
        "                chunks.append(\"\\n\".join(current_chunk_lines).strip())\n",
        "            current_chunk_lines = [cleaned_line]\n",
        "            header_chunk_finished = True\n",
        "        elif is_heading and header_chunk_finished:\n",
        "            if current_chunk_lines:\n",
        "                chunks.append(\"\\n\".join(current_chunk_lines).strip())\n",
        "            current_chunk_lines = [cleaned_line]\n",
        "        else:\n",
        "            if cleaned_line:\n",
        "                current_chunk_lines.append(cleaned_line)\n",
        "\n",
        "    if current_chunk_lines:\n",
        "        chunks.append(\"\\n\".join(current_chunk_lines).strip())\n",
        "\n",
        "    return [chunk for chunk in chunks if len(chunk) > 20]\n",
        "\n",
        "# --- 2. Semantic Embedding and Indexing (Unchanged) ---\n",
        "def build_semantic_index(text_chunks, model):\n",
        "    embeddings = model.encode(text_chunks, convert_to_tensor=True)\n",
        "    embedding_dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(embedding_dim)\n",
        "    index.add(embeddings.cpu().numpy())\n",
        "    return index\n",
        "\n",
        "# --- 3. Semantic Search (Now finds more context) ---\n",
        "def search(query, model, index, text_chunks, top_k=3): # Increased to 3 for richer context\n",
        "    query_embedding = model.encode([query])\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    return [text_chunks[i] for i in indices[0]]\n",
        "\n",
        "# --- 4. Question Answering (UPGRADED to use Gemini API) ---\n",
        "async def answer_question_with_gemini(question, context_chunks):\n",
        "    \"\"\"\n",
        "    Uses the Gemini 2.5 Flash model for superior understanding, summarization, and accuracy.\n",
        "    \"\"\"\n",
        "    if not context_chunks:\n",
        "        return \"I couldn't find any relevant information in the resume.\"\n",
        "\n",
        "    context = \"\\n\\n---\\n\\n\".join(context_chunks)\n",
        "\n",
        "    # A more sophisticated prompt for the powerful LLM\n",
        "    system_prompt = \"\"\"\n",
        "    You are an expert HR assistant. Your task is to answer questions about a candidate's resume accurately and concisely.\n",
        "    - Base your answer STRICTLY on the provided resume context.\n",
        "    - If the information is not present in the context, state that clearly. Do not make assumptions.\n",
        "    - For lists (like skills or projects), present them clearly, preferably using bullet points.\n",
        "    - Keep answers directly relevant to the user's question.\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    CONTEXT:\n",
        "    {context}\n",
        "\n",
        "    QUESTION:\n",
        "    {question}\n",
        "    \"\"\"\n",
        "\n",
        "    api_key = \"\" # This will be handled by the execution environment.\n",
        "    api_url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={api_key}\"\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": user_prompt}]}],\n",
        "        \"systemInstruction\": {\"parts\": [{\"text\": system_prompt}]},\n",
        "        \"generationConfig\": {\n",
        "            \"temperature\": 0.2,\n",
        "            \"topP\": 0.9,\n",
        "            \"maxOutputTokens\": 500,\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # API call with exponential backoff for robustness\n",
        "    max_retries = 5\n",
        "    delay = 1\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = await fetch(api_url, {\n",
        "                \"method\": 'POST',\n",
        "                \"headers\": {'Content-Type': 'application/json'},\n",
        "                \"body\": json.dumps(payload)\n",
        "            })\n",
        "\n",
        "            if response.ok:\n",
        "                result = await response.json()\n",
        "                candidate = result.get('candidates', [{}])[0]\n",
        "                if candidate.get('content', {}).get('parts', [{}])[0].get('text'):\n",
        "                    return candidate['content']['parts'][0]['text']\n",
        "                else:\n",
        "                    return \"Sorry, I received an unexpected response from the AI. Please try again.\"\n",
        "            else:\n",
        "                error_text = await response.text()\n",
        "                print(f\"API Error (Attempt {attempt+1}/{max_retries}): {response.status} - {error_text}\")\n",
        "                if response.status == 429: # Rate limit error\n",
        "                    time.sleep(delay)\n",
        "                    delay *= 2 # Exponential backoff\n",
        "                else:\n",
        "                    return f\"Error: Could not get an answer from the AI. Status: {response.status}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Fetch Error (Attempt {attempt+1}/{max_retries}): {e}\")\n",
        "            time.sleep(delay)\n",
        "            delay *= 2\n",
        "\n",
        "    return \"Sorry, the AI service is currently unavailable after multiple retries. Please try again later.\"\n",
        "\n",
        "\n",
        "# --- Main Application Logic (Updated to be async) ---\n",
        "class ResumeChatbot:\n",
        "    def __init__(self, resume_path):\n",
        "        print(\"Loading embedding model...\")\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        print(\"Model loaded.\")\n",
        "\n",
        "        print(\"Processing resume with precision parser...\")\n",
        "        self.text_chunks = load_and_parse_resume_by_line(resume_path)\n",
        "        if not self.text_chunks:\n",
        "            raise ValueError(f\"Could not extract any text sections from {resume_path}\")\n",
        "\n",
        "        self.faiss_index = build_semantic_index(self.text_chunks, self.embedding_model)\n",
        "        print(f\"Resume indexed into {len(self.text_chunks)} sections.\")\n",
        "\n",
        "    async def query(self, question):\n",
        "        print(f\"\\nReceived question: {question}\")\n",
        "        relevant_chunks = search(question, self.embedding_model, self.faiss_index, self.text_chunks)\n",
        "        print(f\"Found {len(relevant_chunks)} relevant section(s)...\")\n",
        "        answer = await answer_question_with_gemini(question, relevant_chunks)\n",
        "        print(f\"Generated answer: {answer}\")\n",
        "        return answer\n",
        "\n",
        "# --- Gradio Web Interface (Updated to be async) ---\n",
        "def create_chatbot_interface(chatbot_instance):\n",
        "    with gr.Blocks(theme=gr.themes.Soft(), title=\"Resume Chatbot\") as demo:\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            # AI-Powered Resume Chatbot (Gemini Edition)\n",
        "            Ask any question about the resume for an accurate, synthesized answer.\n",
        "            \"\"\"\n",
        "        )\n",
        "        chatbot = gr.Chatbot(label=\"Chat History\", height=500)\n",
        "        msg = gr.Textbox(label=\"Your Question\", placeholder=\"e.g., Summarize their fraud detection project\")\n",
        "        clear = gr.Button(\"Clear Chat\")\n",
        "\n",
        "        async def user(user_message, history):\n",
        "            return \"\", history + [[user_message, None]]\n",
        "\n",
        "        async def bot(history):\n",
        "            user_message = history[-1][0]\n",
        "            bot_message = await chatbot_instance.query(user_message)\n",
        "            history[-1][1] = bot_message\n",
        "            return history\n",
        "\n",
        "        msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
        "            bot, chatbot, chatbot\n",
        "        )\n",
        "        clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "    return demo\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    resume_file_path = \"Nandhini_Resume.pdf\"\n",
        "    try:\n",
        "        # NOTE: Gradio uses 'asyncio' under the hood, so we don't need to manage the event loop.\n",
        "        chatbot_app = ResumeChatbot(resume_file_path)\n",
        "        interface = create_chatbot_interface(chatbot_app)\n",
        "        interface.launch(share=True)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{resume_file_path}' was not found.\")\n",
        "        print(\"Please make sure your resume PDF is in the same directory as this script.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model...\n",
            "Model loaded.\n",
            "Processing resume with precision parser...\n",
            "Resume indexed into 8 sections.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-792781146.py:174: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"Chat History\", height=500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1f40c9e37784f19ad1.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1f40c9e37784f19ad1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "kQZAASY0pB2P",
        "outputId": "e26a992a-04ce-49af-e702-faab0f63c167"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac97fcc0",
        "outputId": "9f837343-8207-4c9c-f246-3a8cb212f846"
      },
      "source": [
        "!pip install PyMuPDF"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.4\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}