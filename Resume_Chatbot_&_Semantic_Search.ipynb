{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NandhiniAnne/nandhiniresumechatbot/blob/main/Resume_Chatbot_%26_Semantic_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nq3cQCWP_wS-",
        "outputId": "47795b12-921a-4f13-b545-3f9739d010d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embedding model...\n",
            "Model loaded.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\annen\\AppData\\Local\\Temp\\ipykernel_17852\\2577160674.py:304: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chat_interface = gr.ChatInterface(assistant.chat_query, chatbot=gr.Chatbot(height=550), textbox=gr.Textbox(placeholder=\"Ask about skills, experience, or compare candidates...\"))\n",
            "C:\\Users\\annen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gradio\\chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://3cef7c38b133b1b5c6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://3cef7c38b133b1b5c6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://3cef7c38b133b1b5c6.gradio.live\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import fitz  # PyMuPDF\n",
        "import json\n",
        "import os\n",
        "import asyncio\n",
        "import aiohttp\n",
        "from dotenv import load_dotenv\n",
        "import numpy as np\n",
        "\n",
        "# --- Load API Key from .env file ---\n",
        "# This line looks for a .env file in your project folder and loads the API key.\n",
        "load_dotenv()\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "# --- Define JSON Schemas for Structured Output ---\n",
        "# This schema tells the AI exactly how to format the candidate summary.\n",
        "SUMMARY_SCHEMA = {\n",
        "    \"type\": \"OBJECT\",\n",
        "    \"properties\": {\n",
        "        \"name\": {\"type\": \"STRING\"},\n",
        "        \"summary\": {\"type\": \"STRING\"},\n",
        "        \"top_skills\": {\n",
        "            \"type\": \"ARRAY\",\n",
        "            \"items\": {\"type\": \"STRING\"}\n",
        "        }\n",
        "    },\n",
        "    \"required\": [\"name\", \"summary\", \"top_skills\"]\n",
        "}\n",
        "\n",
        "# This schema tells the AI exactly how to format the job match score and rationale.\n",
        "MATCH_SCHEMA = {\n",
        "    \"type\": \"OBJECT\",\n",
        "    \"properties\": {\n",
        "        \"candidate_name\": {\"type\": \"STRING\"},\n",
        "        \"match_score\": {\n",
        "            \"type\": \"INTEGER\",\n",
        "            \"description\": \"A score from 0 to 100 representing how well the candidate matches the job description.\"\n",
        "        },\n",
        "        \"rationale\": {\n",
        "            \"type\": \"STRING\",\n",
        "            \"description\": \"A brief, one or two sentence explanation for the score.\"\n",
        "        }\n",
        "    },\n",
        "    \"required\": [\"candidate_name\", \"match_score\", \"rationale\"]\n",
        "}\n",
        "\n",
        "\n",
        "# --- 1. Data Loading and Preprocessing ---\n",
        "def parse_resume_text(full_text, filename):\n",
        "    \"\"\"Parses the full text of a resume into logical sections.\"\"\"\n",
        "    headings = [\n",
        "        'OBJECTIVE', 'EDUCATION', 'SKILLS', 'PROJECTS', 'CERTIFICATIONS',\n",
        "        'EXTRACURRICULAR ACTIVITIES', 'ADDITIONAL INFORMATION', 'WORK EXPERIENCE',\n",
        "        'EXPERIENCE', 'EMPLOYMENT HISTORY', 'PUBLICATIONS', 'SUMMARY', 'CONTACT'\n",
        "    ]\n",
        "    lines = full_text.split('\\n')\n",
        "    chunks = []\n",
        "    \n",
        "    # Capture the header/contact info as the first chunk\n",
        "    header_chunk = []\n",
        "    first_heading_found = False\n",
        "    for line in lines:\n",
        "        cleaned_line = line.strip()\n",
        "        if cleaned_line.upper() in headings:\n",
        "            first_heading_found = True\n",
        "            break\n",
        "        if cleaned_line:\n",
        "            header_chunk.append(cleaned_line)\n",
        "    \n",
        "    if header_chunk:\n",
        "        chunks.append({'source': filename, 'content': \"\\n\".join(header_chunk)})\n",
        "\n",
        "    # Process the rest of the sections\n",
        "    current_chunk_lines = []\n",
        "    if first_heading_found:\n",
        "      # Find where the actual sections start\n",
        "      start_index = 0\n",
        "      for i, line in enumerate(lines):\n",
        "          if line.strip().upper() in headings:\n",
        "              start_index = i\n",
        "              break\n",
        "      \n",
        "      for line in lines[start_index:]:\n",
        "          cleaned_line = line.strip()\n",
        "          is_heading = cleaned_line.upper() in headings\n",
        "          if is_heading:\n",
        "              if current_chunk_lines:\n",
        "                  chunks.append({'source': filename, 'content': \"\\n\".join(current_chunk_lines).strip()})\n",
        "              current_chunk_lines = [cleaned_line]\n",
        "          else:\n",
        "              if cleaned_line:\n",
        "                  current_chunk_lines.append(cleaned_line)\n",
        "\n",
        "    if current_chunk_lines:\n",
        "        chunks.append({'source': filename, 'content': \"\\n\".join(current_chunk_lines).strip()})\n",
        "        \n",
        "    return [chunk for chunk in chunks if len(chunk['content']) > 20]\n",
        "\n",
        "# --- 2. Semantic Embedding and Indexing ---\n",
        "def build_semantic_index(chunks_with_source, model):\n",
        "    if not chunks_with_source: return None, []\n",
        "    text_chunks = [chunk['content'] for chunk in chunks_with_source]\n",
        "    embeddings = model.encode(text_chunks)\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(np.array(embeddings).astype('float32'))\n",
        "    return index, text_chunks\n",
        "\n",
        "# --- 3. Semantic Search ---\n",
        "def search(query, model, index, all_chunks_with_source, top_k=3):\n",
        "    if index is None or index.ntotal == 0: return []\n",
        "    query_embedding = model.encode([query])\n",
        "    distances, indices = index.search(np.array(query_embedding).astype('float32'), top_k)\n",
        "    # Ensure indices are within bounds\n",
        "    valid_indices = [i for i in indices[0] if i < len(all_chunks_with_source)]\n",
        "    return [all_chunks_with_source[i] for i in valid_indices]\n",
        "\n",
        "# --- 4. Gemini API Communication ---\n",
        "async def call_gemini_api(system_prompt, user_prompt, json_schema=None):\n",
        "    if not GEMINI_API_KEY:\n",
        "        return {\"error\": \"API key not found. Please check your .env file and ensure it is in the same folder as the script.\"}\n",
        "\n",
        "    api_url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={GEMINI_API_KEY}\"\n",
        "    \n",
        "    generation_config = {\"temperature\": 0.2, \"topP\": 0.9, \"maxOutputTokens\": 1024}\n",
        "    if json_schema:\n",
        "        generation_config[\"responseMimeType\"] = \"application/json\"\n",
        "        generation_config[\"responseSchema\"] = json_schema\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": user_prompt}]}],\n",
        "        \"systemInstruction\": {\"parts\": [{\"text\": system_prompt}]},\n",
        "        \"generationConfig\": generation_config\n",
        "    }\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        try:\n",
        "            async with session.post(api_url, json=payload, headers={'Content-Type': 'application/json'}) as response:\n",
        "                if response.status == 200:\n",
        "                    result = await response.json()\n",
        "                    candidate = result.get('candidates', [{}])[0]\n",
        "                    part = candidate.get('content', {}).get('parts', [{}])[0]\n",
        "                    if 'text' in part:\n",
        "                        return {\"text\": part['text']}\n",
        "                    return {\"error\": f\"AI returned an empty response. Reason: {candidate.get('finishReason', 'Unknown')}\"}\n",
        "                else:\n",
        "                    error_text = await response.text()\n",
        "                    return {\"error\": f\"API Error {response.status}: {error_text}\"}\n",
        "        except aiohttp.ClientError as e:\n",
        "            return {\"error\": f\"Network Error: {e}\"}\n",
        "\n",
        "# --- Main Application Logic & Gradio UI ---\n",
        "class RecruitingAssistant:\n",
        "    def __init__(self):\n",
        "        print(\"Loading embedding model...\")\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.all_chunks_with_source = []\n",
        "        self.full_resumes = {}\n",
        "        self.summaries = {}\n",
        "        self.faiss_index = None\n",
        "        print(\"Model loaded.\")\n",
        "\n",
        "    async def analyze_and_index_resumes(self, file_paths):\n",
        "        if not file_paths:\n",
        "            return \"Please upload at least one resume.\", gr.HTML(visible=False), gr.Accordion(visible=False)\n",
        "\n",
        "        self.all_chunks_with_source = []\n",
        "        self.full_resumes = {}\n",
        "        self.summaries = {}\n",
        "        \n",
        "        for temp_file in file_paths:\n",
        "            filename = os.path.basename(temp_file.name)\n",
        "            try:\n",
        "                doc = fitz.open(temp_file.name)\n",
        "                full_text = \"\".join(page.get_text() for page in doc)\n",
        "                self.full_resumes[filename] = full_text\n",
        "                chunks = parse_resume_text(full_text, filename)\n",
        "                self.all_chunks_with_source.extend(chunks)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "        \n",
        "        if not self.all_chunks_with_source:\n",
        "            return \"Could not parse any text.\", gr.HTML(visible=False), gr.Accordion(visible=False)\n",
        "\n",
        "        self.faiss_index, _ = build_semantic_index(self.all_chunks_with_source, self.embedding_model)\n",
        "        \n",
        "        # --- Generate Summaries ---\n",
        "        summary_tasks = []\n",
        "        for filename, text in self.full_resumes.items():\n",
        "            prompt = f\"Analyze the following resume text for '{filename}' and provide a summary.\\n\\n{text}\"\n",
        "            system_prompt = \"You are an expert resume analyzer. Extract the candidate's name, a brief summary, and their top 5 skills based on the provided text.\"\n",
        "            summary_tasks.append(call_gemini_api(system_prompt, prompt, SUMMARY_SCHEMA))\n",
        "        \n",
        "        results = await asyncio.gather(*summary_tasks)\n",
        "        \n",
        "        summary_html = \"<div>\"\n",
        "        for i, result in enumerate(results):\n",
        "            filename = list(self.full_resumes.keys())[i]\n",
        "            if \"error\" in result:\n",
        "                self.summaries[filename] = {\"error\": result[\"error\"]}\n",
        "                summary_html += f\"\"\"\n",
        "                <div style='border: 1px solid #ddd; padding: 15px; margin-bottom: 15px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);'>\n",
        "                    <h3 style='margin:0 0 10px 0;'>{filename}</h3>\n",
        "                    <p style='color: red;'><strong>Error:</strong> {result['error']}</p>\n",
        "                </div>\n",
        "                \"\"\"\n",
        "            else:\n",
        "                try:\n",
        "                    summary_data = json.loads(result['text'])\n",
        "                    self.summaries[filename] = summary_data\n",
        "                    skills_html = \"\".join([f\"<span style='background-color: #e0e7ff; color: #4338ca; padding: 3px 8px; border-radius: 12px; font-size: 0.9em; margin: 2px;'>{skill}</span>\" for skill in summary_data.get('top_skills', [])])\n",
        "                    summary_html += f\"\"\"\n",
        "                    <div style='border: 1px solid #ddd; padding: 15px; margin-bottom: 15px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);'>\n",
        "                        <h3 style='margin:0 0 10px 0;'>{summary_data.get('name', 'N/A')} <span style='font-size:0.8em; color:#666;'>({filename})</span></h3>\n",
        "                        <p style='margin:0 0 10px 0;'>{summary_data.get('summary', 'No summary available.')}</p>\n",
        "                        <div style='display: flex; flex-wrap: wrap; gap: 5px;'>{skills_html}</div>\n",
        "                    </div>\n",
        "                    \"\"\"\n",
        "                except json.JSONDecodeError:\n",
        "                     self.summaries[filename] = {\"error\": \"Invalid JSON response from AI.\"}\n",
        "\n",
        "        summary_html += \"</div>\"\n",
        "        \n",
        "        return f\"Indexed {len(self.full_resumes)} resumes.\", gr.HTML(summary_html, visible=True), gr.Accordion(label=\"Candidate Summaries\", open=True, visible=True)\n",
        "\n",
        "    async def chat_query(self, question, history):\n",
        "        if self.faiss_index is None:\n",
        "            return \"Please upload and index resumes first.\"\n",
        "        relevant_chunks = search(question, self.embedding_model, self.faiss_index, self.all_chunks_with_source)\n",
        "        if not relevant_chunks:\n",
        "             return \"No relevant information found.\"\n",
        "        \n",
        "        context = \"\\n\\n---\\n\\n\".join([f\"CONTEXT from resume '{c['source']}':\\n{c['content']}\" for c in relevant_chunks])\n",
        "        user_prompt = f\"{context}\\n\\nQUESTION: {question}\"\n",
        "        system_prompt = \"You are an expert HR assistant. Answer questions strictly based on the provided resume context. If information is missing, state that clearly. For lists like skills, use bullet points.\"\n",
        "        \n",
        "        result = await call_gemini_api(system_prompt, user_prompt)\n",
        "        return result.get('text', result.get('error', 'An unknown error occurred.'))\n",
        "\n",
        "    async def match_job_description(self, job_description):\n",
        "        if not self.full_resumes:\n",
        "            return \"Please upload and analyze resumes first.\", gr.HTML(visible=False)\n",
        "\n",
        "        match_tasks = []\n",
        "        for filename, text in self.full_resumes.items():\n",
        "            user_prompt = f\"Job Description:\\n{job_description}\\n\\n---\\n\\nCandidate Resume ('{filename}'):\\n{text}\"\n",
        "            system_prompt = \"You are an expert recruiter. Analyze the resume against the job description and return a match score and rationale in JSON format.\"\n",
        "            match_tasks.append(call_gemini_api(system_prompt, user_prompt, MATCH_SCHEMA))\n",
        "            \n",
        "        results = await asyncio.gather(*match_tasks)\n",
        "        \n",
        "        matches = []\n",
        "        for result in results:\n",
        "            if \"error\" not in result:\n",
        "                try:\n",
        "                    match_data = json.loads(result['text'])\n",
        "                    matches.append(match_data)\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "        \n",
        "        # Sort by score descending\n",
        "        matches.sort(key=lambda x: x.get('match_score', 0), reverse=True)\n",
        "        \n",
        "        match_html = \"<div>\"\n",
        "        for match in matches:\n",
        "            score = match.get('match_score', 0)\n",
        "            color = \"#dc2626\" if score < 50 else (\"#f59e0b\" if score < 75 else \"#16a34a\")\n",
        "            match_html += f\"\"\"\n",
        "            <div style='border: 1px solid #ddd; padding: 15px; margin-bottom: 15px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);'>\n",
        "                <div style='display: flex; justify-content: space-between; align-items: center;'>\n",
        "                    <h3 style='margin:0;'>{match.get('candidate_name', 'N/A')}</h3>\n",
        "                    <div style='text-align: right;'>\n",
        "                        <span style='font-size: 1.2em; font-weight: bold; color: {color};'>{score}/100</span>\n",
        "                        <div style='width: 100px; background-color: #e5e7eb; border-radius: 5px; margin-top: 5px;'><div style='width: {score}%; height: 8px; background-color: {color}; border-radius: 5px;'></div></div>\n",
        "                    </div>\n",
        "                </div>\n",
        "                <p style='margin-top: 10px; font-style: italic; color: #333;'><strong>Rationale:</strong> {match.get('rationale', 'No rationale provided.')}</p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "        match_html += \"</div>\"\n",
        "        \n",
        "        return f\"Matching complete for {len(matches)} candidates.\", gr.HTML(match_html, visible=True)\n",
        "\n",
        "\n",
        "def create_ui():\n",
        "    assistant = RecruitingAssistant()\n",
        "    with gr.Blocks(theme=gr.themes.Soft(), title=\"AI Recruiting Assistant\") as demo:\n",
        "        gr.Markdown(\"# AI Recruiting Assistant\")\n",
        "        gr.Markdown(\"Upload resumes, get instant summaries, and match candidates to job descriptions.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                file_uploader = gr.File(label=\"Upload Resumes (PDFs)\", file_count=\"multiple\", file_types=[\".pdf\"])\n",
        "                index_button = gr.Button(\"Analyze & Index Resumes\", variant=\"primary\")\n",
        "                status_output = gr.Markdown(value=\"*No resumes indexed yet.*\")\n",
        "                \n",
        "                with gr.Accordion(\"Candidate Summaries\", open=False, visible=False) as summary_accordion:\n",
        "                    summary_display = gr.HTML()\n",
        "            \n",
        "            with gr.Column(scale=2):\n",
        "                with gr.Tabs():\n",
        "                    with gr.TabItem(\"Chat with Resumes\"):\n",
        "                        chat_interface = gr.ChatInterface(assistant.chat_query, chatbot=gr.Chatbot(height=550), textbox=gr.Textbox(placeholder=\"Ask about skills, experience, or compare candidates...\"))\n",
        "                    with gr.TabItem(\"Job Matcher\"):\n",
        "                        with gr.Column():\n",
        "                            job_desc_box = gr.Textbox(lines=10, label=\"Job Description\", placeholder=\"Paste the full job description here...\")\n",
        "                            match_button = gr.Button(\"Match to Job Description\", variant=\"primary\")\n",
        "                            match_status = gr.Markdown(\"*Matching results will appear below.*\")\n",
        "                            match_results_display = gr.HTML(visible=False)\n",
        "\n",
        "        index_button.click(\n",
        "            assistant.analyze_and_index_resumes,\n",
        "            inputs=[file_uploader],\n",
        "            outputs=[status_output, summary_display, summary_accordion]\n",
        "        )\n",
        "        \n",
        "        match_button.click(\n",
        "            assistant.match_job_description,\n",
        "            inputs=[job_desc_box],\n",
        "            outputs=[match_status, match_results_display]\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not GEMINI_API_KEY or len(GEMINI_API_KEY) < 10:\n",
        "        print(\"=\"*60)\n",
        "        print(\"--- WARNING: GEMINI_API_KEY NOT FOUND OR INVALID ---\")\n",
        "        print(\"Please create a file named '.env' in the same folder as this script.\")\n",
        "        print(\"In that file, add one line: \")\n",
        "        print('GEMINI_API_KEY=\"YOUR_ACTUAL_API_KEY_HERE\"')\n",
        "        print(\"You can get a key from Google AI Studio.\")\n",
        "        print(\"=\"*60)\n",
        "    \n",
        "    app = create_ui()\n",
        "    app.launch(share=True, debug=True)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
